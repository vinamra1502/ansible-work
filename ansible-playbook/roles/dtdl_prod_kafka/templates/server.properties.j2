
broker.id={{ kafka_broker_id }}


super.users=User:{{ kafka_super_user }}

listeners=EXTERNAL://:{{ kafka_lb_port }},INTERNAL://0.0.0.0:{{ kafka_broker_port }}

advertised.listeners=EXTERNAL://{{ kafka_lb_dns }}:{{ kafka_lb_port }},INTERNAL://{{ kafka_ip }}:{{ kafka_broker_port }}

listener.security.protocol.map=INTERNAL:{{ kafka_listner_security_protocol }},EXTERNAL:{{ kafka_listner_security_protocol }}

inter.broker.listener.name=INTERNAL

authorizer.class.name=kafka.security.auth.SimpleAclAuthorizer

sasl.mechanism.inter.broker.protocol={{ kafka_sasl_internal_broker_protocol }}

sasl.enabled.mechanisms={{ kafka_sasl_mechanism }}

# The number of threads that the server uses for receiving requests from the network and sending responses to the network
num.network.threads=3

# The number of threads that the server uses for processing requests, which may include disk I/O
num.io.threads=8

# The send buffer (SO_SNDBUF) used by the socket server
socket.send.buffer.bytes=102400

# The receive buffer (SO_RCVBUF) used by the socket server
socket.receive.buffer.bytes=102400

# The maximum size of a request that the socket server will accept (protection against OOM)
socket.request.max.bytes=104857600




log.dirs={{ kafka_data_dir }}

num.partitions=1


num.recovery.threads.per.data.dir=1


offsets.topic.replication.factor=1
transaction.state.log.replication.factor=1
transaction.state.log.min.isr=1



# The number of messages to accept before forcing a flush of data to disk
#log.flush.interval.messages=10000

# The maximum amount of time a message can sit in a log before we force a flush
#log.flush.interval.ms=1000

log.retention.hours=168


#log.retention.bytes=1073741824

# The maximum size of a log segment file. When this size is reached a new log segment will be created.
log.segment.bytes=1073741824


log.retention.check.interval.ms=300000


zookeeper.connect={{ kafka_zookeeper_list }}
# Timeout in ms for connecting to zookeeper
zookeeper.connection.timeout.ms=6000



group.initial.rebalance.delay.ms=0
